{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "070cbd9f",
   "metadata": {},
   "source": [
    "### Classification techniques. ML in seismic methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5fdc2ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "11bf1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lasio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb0955aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc35ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload .las files (fies for log data saving)\n",
    "\n",
    "las_files = []\n",
    "\n",
    "for p in glob.glob('data/*.las'):\n",
    "    las_files.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a9612f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Well-log names (headers)\n",
    "f_cur = 'KOL'\n",
    "dgk_cur = 'DGR'\n",
    "aps_cur = 'ASP_EST'\n",
    "d_cur = 'MD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "18d52703",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arrays for writing out data\n",
    "facies = []\n",
    "dgk_log = []\n",
    "aps_log = []\n",
    "depth_log = []\n",
    "well_num = []\n",
    "\n",
    "\n",
    "# Read data from each file and add logs to lists\n",
    "for item in range(len(las_files)):\n",
    "    las = lasio.read(las_files[item])\n",
    "    dgk = las[dgk_cur]\n",
    "    aps = las[aps_cur]\n",
    "    depth = las[d_cur]\n",
    "    ff = las[f_cur]\n",
    "    \n",
    "    # Deleting empty samples from logs\n",
    "    dgk = dgk[~np.isnan(ff)]\n",
    "    aps = aps[~np.isnan(ff)]\n",
    "    depth = depth[~np.isnan(ff)]\n",
    "    ff = ff[~np.isnan(ff)]\n",
    "    \n",
    "    depth = depth[~np.isnan(aps)]\n",
    "    ff = ff[~np.isnan(aps)]\n",
    "    dgk = dgk[~np.isnan(aps)]\n",
    "    aps = aps[~np.isnan(aps)]\n",
    "    \n",
    "    depth = depth[~np.isnan(dgk)]\n",
    "    ff = ff[~np.isnan(dgk)]\n",
    "    aps = aps[~np.isnan(dgk)]\n",
    "    dgk = dgk[~np.isnan(dgk)]\n",
    "    \n",
    "    facies.append(ff)\n",
    "    dgk_log.append(dgk)\n",
    "    aps_log.append(aps)\n",
    "    depth_log.append(depth)\n",
    "    well_num.append(np.ones(len(ff))*item)\n",
    "\n",
    "# Merge data into one array\n",
    "dgk_log = np.concatenate(dgk_log)\n",
    "aps_log = np.concatenate(aps_log)\n",
    "depth_log = np.concatenate(depth_log)\n",
    "facies = np.concatenate(facies)\n",
    "well_num = np.concatenate(well_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eac5441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Normalisation (we have two well logs providing physical data)\n",
    "from sklearn import preprocessing\n",
    "\n",
    "min_max_scaler_1 = preprocessing.MinMaxScaler()\n",
    "min_max_scaler_2 = preprocessing.MinMaxScaler()\n",
    "\n",
    "Xn_1 = min_max_scaler_1.fit_transform(dgk_log.reshape(-1, 1))\n",
    "Xn_2 = min_max_scaler_2.fit_transform(aps_log.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "68641db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutting off most informative part\n",
    "X = np.hstack((Xn_1[(depth_log>1590) & (depth_log<1750)], Xn_2[(depth_log>1590) & (depth_log<1750)]))\n",
    "\n",
    "y = facies[(depth_log>1590) & (depth_log<1750)].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bfe14119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GaussianNB\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.93      0.93      1546\n",
      "           1       0.74      0.78      0.76       414\n",
      "\n",
      "    accuracy                           0.90      1960\n",
      "   macro avg       0.84      0.85      0.85      1960\n",
      "weighted avg       0.90      0.90      0.90      1960\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=11)\n",
    "\n",
    "# примерение различных классификаторов\n",
    "\n",
    "gnb = GaussianNB() \n",
    "y_pred = gnb.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print('GaussianNB')\n",
    "print(metrics.classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107603e4",
   "metadata": {},
   "source": [
    "#### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "0388a58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bayes classifier\n",
    "\n",
    "# create frequency histogramm (reservoir possibility)\n",
    "def create_histogramm(positive_values_list, negative_values_list, number_of_columns):\n",
    "        borders = [0, ]\n",
    "        positive_probabilities = []\n",
    "        negative_probabilities = []\n",
    "        step = 1 / number_of_columns\n",
    "        \n",
    "        lower_border = 0\n",
    "        x = True\n",
    "        while x: \n",
    "            # from 0 to 1 \n",
    "            # Ones are used in order to avoid Gaussian algos difficulties\n",
    "            number_of_elements_p_column = 1\n",
    "            number_of_elements_n_column = 1\n",
    "            upper_border = lower_border + step\n",
    "            borders.append(upper_border)\n",
    "            \n",
    "            for i in positive_values_list:\n",
    "                if i > lower_border and i <= upper_border:\n",
    "                    number_of_elements_p_column += 1\n",
    "                    \n",
    "            for j in negative_values_list:\n",
    "                if j > lower_border and j <= upper_border:\n",
    "                    number_of_elements_n_column += 1\n",
    "                    \n",
    "            lower_border = upper_border\n",
    "            \n",
    "            positive_probabilities.append( number_of_elements_p_column )\n",
    "            negative_probabilities.append( number_of_elements_n_column )\n",
    "            \n",
    "            if upper_border >= 1:\n",
    "                 x = False\n",
    "        return borders, positive_probabilities, negative_probabilities\n",
    "    \n",
    "\n",
    "\n",
    "def bayes_learn(normalised_values, number_of_columns, res):\n",
    "    histograms_borders = []\n",
    "    histograms_p_probabilities = []\n",
    "    histograms_n_probabilities = []\n",
    "    \n",
    "    # indexes, where reservoir-layer is located\n",
    "    positive_indexes = [index for index in range(len(res)) if res[index] == 1]\n",
    "    # indexes, where non-reservoir-layer is located\n",
    "    negative_indexes = [index for index in range(len(res)) if res[index] == 0]\n",
    "    p_values = normalised_values[positive_indexes]\n",
    "    n_values = normalised_values[negative_indexes]\n",
    "    borders, p_probabilities, n_probabilities = create_histogramm(p_values, n_values, number_of_columns)\n",
    "    # returns result for one parameter    \n",
    "    return borders, p_probabilities, n_probabilities\n",
    "        \n",
    "       \n",
    "        \n",
    "\n",
    "def bayes_predict(par1, par2, reservour_class_possibity, non_reservoir_class_possibility):\n",
    "    if par1 == 0: par1 += 10e-9 \n",
    "    if par2 == 0: par2 += 10e-9 \n",
    "    # First parameter\n",
    "    \n",
    "    positive_first_hist_sum = sum(histograms_p_probabilities1)\n",
    "    negative_first_hist_sum = sum(histograms_n_probabilities1)\n",
    "    full_first_sum = positive_first_hist_sum + negative_first_hist_sum\n",
    "    \n",
    "    for i in range(len(histograms_borders1)):\n",
    "        if par1 > histograms_borders1[i] and par1 <= histograms_borders1[i+1]:\n",
    "            prob_to_others1_p = histograms_p_probabilities1[i] / positive_first_hist_sum\n",
    "            prob_to_others1_n = histograms_n_probabilities1[i] / negative_first_hist_sum\n",
    "            full_prob1 = ( histograms_p_probabilities1[i] + histograms_n_probabilities1[i] ) / full_first_sum\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    # Second parameter\n",
    "    \n",
    "    positive_second_hist_sum = sum(histograms_p_probabilities2)\n",
    "    negative_second_hist_sum = sum(histograms_n_probabilities2)\n",
    "    full_second_sum = positive_second_hist_sum + negative_second_hist_sum\n",
    "    \n",
    "    for i in range(len(histograms_borders2)):\n",
    "        if par2 > histograms_borders2[i] and par2 <= histograms_borders2[i+1]:\n",
    "            prob_to_others2_p = histograms_p_probabilities2[i] / positive_second_hist_sum\n",
    "            prob_to_others2_n = histograms_n_probabilities2[i] / negative_second_hist_sum\n",
    "            full_prob2 = ( histograms_p_probabilities2[i] + histograms_n_probabilities2[i] ) / full_second_sum\n",
    "    \n",
    "    prob_resevoir = ( prob_to_others1_p * prob_to_others2_p * reservour_class_possibity ) / (full_prob1 * full_prob2)\n",
    "    prob_non_resevoir = ( prob_to_others1_n * prob_to_others2_n * non_reservoir_class_possibility ) / (full_prob1 * full_prob2)\n",
    "    \n",
    "    \n",
    "    if prob_resevoir > prob_non_resevoir:\n",
    "        return 1\n",
    "    else: \n",
    "        return 0\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "b9952ece",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.21765756570553713 0.7823424342944628\n"
     ]
    }
   ],
   "source": [
    "whole_samples = len(y)\n",
    "class_non_reservoir = list(y).count(0)\n",
    "class_reservoir = whole_samples - class_non_reservoir\n",
    "\n",
    "reservour_class_possibity = class_reservoir / whole_samples\n",
    "non_reservoir_class_possibility = class_non_reservoir / whole_samples\n",
    "\n",
    "print(reservour_class_possibity, non_reservoir_class_possibility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "a268e0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model learning \n",
    "\n",
    "# First parameter - GR\n",
    "\n",
    "histograms_borders1, histograms_p_probabilities1, histograms_n_probabilities1 = bayes_learn(X_train[:, 0], 50, y_train)\n",
    "\n",
    "# Second parameter - PS\n",
    "\n",
    "histograms_borders2, histograms_p_probabilities2, histograms_n_probabilities2 = bayes_learn(X_train[:, 1], 50, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0ef3657b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "\n",
    "pv = bayes_predict(X_test[2, 0], X_test[2, 1], reservour_class_possibity, non_reservoir_class_possibility)\n",
    "print(pv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "bf336c49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.4988203 , 0.3709421 ],\n",
       "       [0.34625   , 0.        ],\n",
       "       [0.340625  , 0.4571485 ],\n",
       "       ...,\n",
       "       [0.8096874 , 0.        ],\n",
       "       [0.5914925 , 0.09621976],\n",
       "       [0.4723231 , 0.4826989 ]])"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "d3928edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction characteristics - Precision and Recall\n",
    "\n",
    "def precision_test(X_test, y_test):\n",
    "    class_nr = list(y_test).count(0)\n",
    "    class_r = list(y_test).count(1)\n",
    "    \n",
    "    # Positive\n",
    "    gotit = 0\n",
    "    got1 = 0\n",
    "    for i in range(len(y_test)):\n",
    "        pv = bayes_predict(X_test[i, 0], X_test[i, 1], reservour_class_possibity, non_reservoir_class_possibility)\n",
    "        \n",
    "        if pv == 1:\n",
    "            got1 += 1\n",
    "        \n",
    "        if pv == y_test[i] and y_test[i] == 1:\n",
    "            gotit += 1\n",
    "    p_positive = gotit / got1  \n",
    "    # Negative\n",
    "    gotit = 0\n",
    "    got1 = 0\n",
    "    for i in range(len(y_test)):\n",
    "        pv = bayes_predict(X_test[i, 0], X_test[i, 1], reservour_class_possibity, non_reservoir_class_possibility)\n",
    "        \n",
    "        if pv == 0:\n",
    "            got1 += 1\n",
    "        \n",
    "        if pv == y_test[i] and y_test[i] == 0:\n",
    "            gotit += 1\n",
    "    p_negative = gotit / got1   \n",
    "    \n",
    "    return p_positive, p_negative\n",
    "    \n",
    "    \n",
    "def recall_test(X_test, y_test):\n",
    "    class_nr = list(y_test).count(0)\n",
    "    class_r = list(y_test).count(1)\n",
    "    \n",
    "    # Positive\n",
    "    gotit = 0\n",
    "    got1 = 0\n",
    "    for i in range(len(y_test)):\n",
    "        pv = bayes_predict(X_test[i, 0], X_test[i, 1], reservour_class_possibity, non_reservoir_class_possibility)\n",
    "        if pv == y_test[i] and y_test[i] == 1:\n",
    "            gotit += 1\n",
    "    p_positive = gotit / class_r  \n",
    "    # Negative\n",
    "    gotit = 0\n",
    "    got1 = 0\n",
    "    for i in range(len(y_test)):\n",
    "        pv = bayes_predict(X_test[i, 0], X_test[i, 1], reservour_class_possibity, non_reservoir_class_possibility)\n",
    "        if pv == y_test[i] and y_test[i] == 0:\n",
    "            gotit += 1\n",
    "    p_negative = gotit / class_nr  \n",
    "    \n",
    "    return p_positive, p_negative\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "fce23392",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision (1): 0.7912371134020618\n",
      "Precision (0): 0.9319338422391857\n",
      "Recall (1): 0.7415458937198067\n",
      "Recall (0): 0.9476067270375161\n"
     ]
    }
   ],
   "source": [
    "presition_positive, presition_negative = precision_test(X_test, y_test)\n",
    "\n",
    "recall_positive, recall_negative = recall_test(X_test, y_test)\n",
    "\n",
    "print('Precision (1):', presition_positive)\n",
    "print('Precision (0):', presition_negative)\n",
    "print('Recall (1):', recall_positive)\n",
    "print('Recall (0):', recall_negative)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21abd3e8",
   "metadata": {},
   "source": [
    "...Within next versions of Bayes Naive Classificator we can use Gaussian function for probability approximation instead of histogramms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
